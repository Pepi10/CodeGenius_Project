{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "import pandas as pd\n",
    "\n",
    "model_name = 'skt/kogpt2-base-v2'\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, base_model_name):\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(base_model_name)\n",
    "    \n",
    "    # 토큰 초기화 함수\n",
    "    def initialize_tokens(self, new_vocab):\n",
    "        self.tokenizer = GPT2TokenizerFast()  # 새로운 토크나이저 생성\n",
    "        self.tokenizer.add_tokens(new_vocab)\n",
    "        print(\"토크나이저가 초기화되었습니다.\")\n",
    "    \n",
    "    # 새로운 토큰 추가 함수\n",
    "    def add_new_tokens(self, new_tokens):\n",
    "        self.tokenizer.add_tokens(new_tokens)\n",
    "        print(\"새로운 토큰이 추가된 후 단어 집합 크기:\", len(self.tokenizer))\n",
    "    \n",
    "    # 토큰 저장 함수\n",
    "    def save_tokenizer(self, save_path):\n",
    "        self.tokenizer.save_pretrained(save_path)\n",
    "        print(f\"토크나이저가 {save_path}에 저장되었습니다.\")\n",
    "    \n",
    "    # 문장 토큰화 함수\n",
    "    def tokenize_sentence(self, sentence):\n",
    "        tokens = self.tokenizer(sentence, return_tensors='pt')\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "import pandas as pd\n",
    "\n",
    "model_name = 'skt/kogpt2-base-v2'\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, base_model_name):\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(base_model_name)\n",
    "    \n",
    "    # 토큰 초기화 함수\n",
    "    def initialize_tokens(self, new_vocab):\n",
    "        # 새로운 토크나이저 생성\n",
    "        self.tokenizer = PreTrainedTokenizerFast()  \n",
    "        self.tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "        self.tokenizer.add_tokens(new_vocab)\n",
    "        print(\"토크나이저가 초기화되었습니다.\")\n",
    "    \n",
    "    # 새로운 토큰 추가 함수\n",
    "    def add_new_tokens(self, new_tokens):\n",
    "        self.tokenizer.add_tokens(new_tokens)\n",
    "        print(\"새로운 토큰이 추가된 후 단어 집합 크기:\", len(self.tokenizer))\n",
    "    \n",
    "    # 토큰 저장 함수\n",
    "    def save_tokenizer(self, save_path):\n",
    "        self.tokenizer.save_pretrained(save_path)\n",
    "        print(f\"토크나이저가 {save_path}에 저장되었습니다.\")\n",
    "    \n",
    "    # 문장 토큰화 함수\n",
    "    def tokenize_sentence(self, sentence):\n",
    "        tokens = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 예시\n",
    "new_vocab = [\"Hello\", \"world\", \"I\", \"am\", \"a\", \"new\", \"tokenizer\", \n",
    "             \"specific\", \"domain\", \"specialized\", \"tokens\"]\n",
    "new_tokens = [\"<NEW_TOKEN_1>\", \"<NEW_TOKEN_2>\", \"<DOMAIN_SPECIFIC_TOKEN>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "custom_tokenizer = CustomTokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'hello world i am a new tokenizer specific domain specialized tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcustom_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_vocab\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mCustomTokenizer.initialize_tokens\u001b[0;34m(self, new_vocab)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, new_vocab):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# 새로운 토크나이저 생성\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mPreTrainedTokenizerFast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39madd_special_tokens({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39madd_tokens(new_vocab)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:127\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer \u001b[38;5;241m=\u001b[39m fast_tokenizer\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "custom_tokenizer.initialize_tokens(new_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tokenizer.add_new_tokens(new_tokens)\n",
    "custom_tokenizer.save_tokenizer('./custom_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 데이터프레임\n",
    "data = {'sentence': [\"Hello, I am testing the new tokenizer.\",\n",
    "                     \"This is another example sentence.\",\n",
    "                     \"Let's see how this works with multiple sentences.\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 데이터프레임의 각 문장을 토큰화\n",
    "df['tokens'] = df['sentence'].apply(lambda x: custom_tokenizer.tokenize_sentence(x))\n",
    "\n",
    "# 결과 출력\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
