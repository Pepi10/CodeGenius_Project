{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_209104/442176259.py:105: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)\n",
      "/tmp/ipykernel_209104/442176259.py:105: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n",
      "end0\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# from pytorch_lightning import Trainer\n",
    "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# from pytorch_lightning.core.lightning import LightningModule\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import re\n",
    "\n",
    "Q_TKN = \"<usr>\"  # 질문 토큰\n",
    "A_TKN = \"<sys>\"  # 답변 토큰\n",
    "BOS = '</s>'  # 문장 시작 토큰\n",
    "EOS = '</s>'  # 문장 끝 토큰\n",
    "MASK = '<mask>'  # 마스킹작업 토큰\n",
    "SENT = '<sent>'  # 문장 구분 토큰\n",
    "PAD = '<pad>'  # 패딩 토큰\n",
    "UNK = '<unk>'  # 사전에 없는 토큰\n",
    "\n",
    "# 모델 및 토큰 불러오기\n",
    "koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "            bos_token=BOS, eos_token=EOS, unk_token=UNK,\n",
    "            pad_token=PAD, mask_token=MASK) \n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# 데이터 불러오기\n",
    "Chatbot_Data = pd.read_csv(\"rawdata.csv\", index_col=0)\n",
    "Chatbot_Data = Chatbot_Data[['question', 'answer']]\n",
    "\n",
    "# 챗봇 데이터를 처리하는 클래스를 만든다.\n",
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, chats, max_len=70):  # 데이터셋의 전처리를 해주는 부분\n",
    "        self._data = chats\n",
    "        self.max_len = max_len\n",
    "        self.q_token = Q_TKN\n",
    "        self.a_token = A_TKN\n",
    "        self.sent_token = SENT\n",
    "        self.eos = EOS\n",
    "        self.mask = MASK\n",
    "        self.tokenizer = koGPT2_TOKENIZER\n",
    "\n",
    "    def __len__(self):  # chatbotdata 의 길이를 리턴한다.\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):  # 로드한 챗봇 데이터를 차례차례 DataLoader로 넘겨주는 메서드\n",
    "        turn = self._data.iloc[idx]\n",
    "        q = turn[\"question\"]  # 질문을 가져온다.\n",
    "        q = re.sub(r\"([?.!,])\", r\" \", q)  # 구둣점들을 제거한다.\n",
    "\n",
    "        a = turn[\"answer\"]  # 답변을 가져온다.\n",
    "        a = re.sub(r\"([?.!,])\", r\" \", a)  # 구둣점들을 제거한다.\n",
    "\n",
    "        q_toked = self.tokenizer.tokenize(self.q_token + q + self.sent_token)\n",
    "        q_len = len(q_toked)\n",
    "\n",
    "        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)\n",
    "        a_len = len(a_toked)\n",
    "\n",
    "        #질문의 길이가 최대길이보다 크면\n",
    "        if q_len > self.max_len:\n",
    "            a_len = self.max_len - q_len        #답변의 길이를 최대길이 - 질문길이\n",
    "            if a_len <= 0:       #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n",
    "                q_toked = q_toked[-(int(self.max_len / 2)) :]   #질문길이를 최대길이의 반으로 \n",
    "                q_len = len(q_toked)\n",
    "                a_len = self.max_len - q_len              #답변의 길이를 최대길이 - 질문길이\n",
    "            a_toked = a_toked[:a_len]\n",
    "            a_len = len(a_toked)\n",
    "\n",
    "        #질문의 길이 + 답변의 길이가 최대길이보다 크면\n",
    "        if q_len + a_len > self.max_len:\n",
    "            a_len = self.max_len - q_len        #답변의 길이를 최대길이 - 질문길이\n",
    "            if a_len <= 0:       #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n",
    "                q_toked = q_toked[-(int(self.max_len / 2)) :]   #질문길이를 최대길이의 반으로 \n",
    "                q_len = len(q_toked)\n",
    "                a_len = self.max_len - q_len              #답변의 길이를 최대길이 - 질문길이\n",
    "            a_toked = a_toked[:a_len]\n",
    "            a_len = len(a_toked)\n",
    "\n",
    "        # 답변 labels = [mask, mask, ...., mask, ..., <bos>,..답변.. <eos>, <pad>....]\n",
    "        labels = [self.mask,] * q_len + a_toked[1:]\n",
    "\n",
    "        # mask = 질문길이 0 + 답변길이 1 + 나머지 0\n",
    "        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)\n",
    "        # 답변 labels을 index 로 만든다.\n",
    "        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)\n",
    "        # 최대길이만큼 PADDING\n",
    "        while len(labels_ids) < self.max_len:\n",
    "            labels_ids += [self.tokenizer.pad_token_id]\n",
    "\n",
    "        # 질문 + 답변을 index 로 만든다.    \n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)\n",
    "        # 최대길이만큼 PADDING\n",
    "        while len(token_ids) < self.max_len:\n",
    "            token_ids += [self.tokenizer.pad_token_id]\n",
    "\n",
    "        #질문+답변, 마스크, 답변\n",
    "        return (token_ids, np.array(mask), labels_ids)\n",
    "    \n",
    "def collate_batch(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    mask = [item[1] for item in batch]\n",
    "    label = [item[2] for item in batch]\n",
    "    return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_set = ChatbotDataset(Chatbot_Data, max_len=100)\n",
    "#윈도우 환경에서 num_workers 는 무조건 0으로 지정, 리눅스에서는 2\n",
    "train_dataloader = DataLoader(train_set, batch_size=32, num_workers=2, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "learning_rate = 3e-5\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epoch = 10\n",
    "Sneg = -1e18\n",
    "\n",
    "print(\"start\")\n",
    "for epoch in range(epoch):\n",
    "    for batch_idx, samples in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 텐서를 적절한 장치로 이동시킵니다.\n",
    "        token_ids, mask, label = samples\n",
    "        token_ids = token_ids.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        # 순전파\n",
    "        out = model(token_ids)\n",
    "        out = out.logits  # 로짓 값을 반환하는 새로운 텐서를 얻습니다.\n",
    "        \n",
    "        # 추가 작업 수행\n",
    "        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)\n",
    "        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out).to(device))  # Sneg의 장치도 동일하게 설정\n",
    "        \n",
    "        loss = criterion(mask_out.transpose(2, 1), label)\n",
    "        \n",
    "        # 평균 loss 계산 및 역전파\n",
    "        avg_loss = loss.sum() / mask.sum()\n",
    "        avg_loss.backward()\n",
    "        \n",
    "        # 학습 끝\n",
    "        optimizer.step()\n",
    "    print(f\"end{epoch}\")\n",
    "print(\"end\")\n",
    "\n",
    "# 모델을 저장할 경로를 지정합니다.\n",
    "model_save_path = 'kogpt2_chatbot_model.pth'\n",
    "\n",
    "# 모델 상태 저장\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epoch,\n",
    "    'train_loss': avg_loss,\n",
    "    # 다른 필요한 상태가 있다면 여기에 추가할 수 있습니다.\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_loader import ModelLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"skt/kogpt2-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<model_loader.ModelLoader at 0x7fcd1d9ede40>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelLoader(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    while 1:\n",
    "        q = input(\"user > \").strip()\n",
    "        if q == \"quit\":\n",
    "            break\n",
    "        a = \"\"\n",
    "        while 1:\n",
    "            input_ids = torch.LongTensor(koGPT2_TOKENIZER.encode(Q_TKN + q + SENT + sent + A_TKN + a)).unsqueeze(dim=0).to(device)\n",
    "            pred = model(input_ids)\n",
    "            logits = pred.logits\n",
    "            logits = logits[:, -1, :]  # 마지막 토큰에 대한 로짓\n",
    "            probs = logits.softmax(dim=-1)\n",
    "            top_prob, top_idx = torch.topk(probs, k=1, dim=-1)  # 최대 확률과 해당 인덱스\n",
    "            gen_token_id = top_idx.item()  # 최대 확률을 가진 토큰 인덱스\n",
    "            gen = koGPT2_TOKENIZER.convert_ids_to_tokens(gen_token_id)\n",
    "            if gen == EOS:\n",
    "                break\n",
    "            a += gen.replace(\"▁\", \" \")\n",
    "        print(\"Chatbot > {}\".format(a.strip()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
