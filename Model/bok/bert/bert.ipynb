{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bok/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 379/379 [01:09<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.367 accuracy 0.475\n",
      "Epoch 2/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 379/379 [01:09<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.354 accuracy 0.515\n",
      "Epoch 3/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 379/379 [01:09<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.357 accuracy 0.484\n",
      "Model saved to ./save/model_v3.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from transformers import BertForSequenceClassification\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_model = 'skt/kobert-base-v1'\n",
    "MODEL_NAME = \"./save/model_v1.pth\"  # 불러올 모델 가중치\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 예제 학습 데이터프레임\n",
    "df = pd.read_excel('./data/train_set.xlsx')\n",
    "\n",
    "texts = df['question'].tolist()\n",
    "df.label = df.label.map({'yes': 1, 'no': 0})\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "max_len = 128\n",
    "batch_size = 2\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained(base_model)\n",
    "dataset = SentimentDataset(texts, labels, tokenizer, max_len)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 모델 로드 및 가중치 초기화\n",
    "model = BertForSequenceClassification.from_pretrained(base_model, num_labels=2)  # 모델의 아키텍처 설정\n",
    "model_save_path = MODEL_NAME  # 가중치 불러오기\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# 학습 설정\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_epoch(model, data_loader, optimizer, device, n_examples):\n",
    "    model = model.train()\n",
    "    losses = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in tqdm(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, losses / n_examples\n",
    "\n",
    "# 학습 실행\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    print('-' * 10)\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        data_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        len(dataset)\n",
    "    )\n",
    "    print(f'Train loss {train_loss:.3f} accuracy {train_acc:.3f}')\n",
    "    \n",
    "# 모델 저장 경로 설정\n",
    "def get_model_save_path(base_path, base_filename, ext):\n",
    "    version = 1\n",
    "    while True:\n",
    "        model_save_path = f\"{base_path}/{base_filename}_v{version}{ext}\"\n",
    "        if not os.path.exists(model_save_path):\n",
    "            return model_save_path\n",
    "        version += 1\n",
    "\n",
    "# 모델 저장\n",
    "base_path = './save'\n",
    "base_filename = 'model'\n",
    "ext = '.pth'\n",
    "\n",
    "model_save_path = get_model_save_path(base_path, base_filename, ext)\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f'Model saved to {model_save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 379/379 [01:09<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.361 accuracy 0.497\n",
      "Validation loss: 0.352, accuracy: 0.500\n",
      "Model saved to ./save/model_v4.pth\n",
      "Epoch 2/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 379/379 [01:09<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.358 accuracy 0.496\n",
      "Validation loss: 0.347, accuracy: 0.500\n",
      "Epoch 3/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 379/379 [01:09<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.358 accuracy 0.489\n",
      "Validation loss: 0.347, accuracy: 0.500\n",
      "Epoch 4/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 379/379 [01:09<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.355 accuracy 0.511\n",
      "Validation loss: 0.361, accuracy: 0.500\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from transformers import BertForSequenceClassification\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_model = 'skt/kobert-base-v1'\n",
    "MODEL_NAME = \"./save/model_v1.pth\"  # 불러올 모델 가중치\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 예제 학습 데이터프레임\n",
    "train_df = pd.read_excel('./data/train_set.xlsx')\n",
    "valid_df = pd.read_excel('./data/valid_label_set.xlsx')\n",
    "\n",
    "train_texts = train_df['question'].tolist()\n",
    "train_df.label = train_df.label.map({'yes': 1, 'no': 0})\n",
    "train_labels = train_df['label'].tolist()\n",
    "\n",
    "valid_texts = valid_df['question'].tolist()\n",
    "valid_df.label = valid_df.label.map({'yes': 1, 'no': 0})\n",
    "valid_labels = valid_df['label'].tolist()\n",
    "\n",
    "max_len = 128\n",
    "batch_size = 2\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained(base_model)\n",
    "\n",
    "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, max_len)\n",
    "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = SentimentDataset(valid_texts, valid_labels, tokenizer, max_len)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 모델 로드 및 가중치 초기화\n",
    "model = BertForSequenceClassification.from_pretrained(base_model, num_labels=2)  # 모델의 아키텍처 설정\n",
    "model_save_path = MODEL_NAME  # 가중치 불러오기\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# 학습 설정\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_epoch(model, data_loader, optimizer, device, n_examples):\n",
    "    model = model.train()\n",
    "    losses = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in tqdm(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, losses / n_examples\n",
    "  \n",
    "# 검증 함수 정의\n",
    "def evaluate(model, data_loader, device):\n",
    "    model = model.eval()\n",
    "    correct_predictions = 0\n",
    "    losses = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses += loss.item()\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), losses / len(data_loader.dataset)  \n",
    "    \n",
    "# 모델 저장 경로 설정\n",
    "def get_model_save_path(base_path, base_filename, ext):\n",
    "    version = 1\n",
    "    while True:\n",
    "        model_save_path = f\"{base_path}/{base_filename}_v{version}{ext}\"\n",
    "        if not os.path.exists(model_save_path):\n",
    "            return model_save_path\n",
    "        version += 1\n",
    "\n",
    "# 조기 종료 설정\n",
    "early_stopping_patience = 3\n",
    "best_accuracy = 0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# 모델 저장 위치 설정\n",
    "base_path = './save'\n",
    "base_filename = 'model'\n",
    "ext = '.pth'\n",
    "\n",
    "# 학습 실행\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    print('-' * 10)\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        data_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        len(dataset)\n",
    "    )\n",
    "    print(f'Train loss {train_loss:.3f} Train accuracy {train_acc:.3f}')\n",
    "    \n",
    "    val_acc, val_loss = evaluate(model, valid_loader, device)\n",
    "    print(f'Validation loss: {val_loss:.3f}, Validation accuracy: {val_acc:.3f}')\n",
    "\n",
    "    # 모델 저장\n",
    "    if val_acc > best_accuracy:\n",
    "        best_accuracy = val_acc\n",
    "        epochs_without_improvement = 0\n",
    "        model_save_path = get_model_save_path(base_path, base_filename, ext)\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f'Model saved to {model_save_path}')\n",
    "        \n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= early_stopping_patience:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid - label 없는 것 붙이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n",
      "100%|██████████| 25/25 [00:01<00:00, 24.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 파이썬에서 리스트와 튜플의 차이는 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 딕셔너리를 어떻게 정의하나요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 클래스는 어떻게 생성하나요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 파일을 읽고 쓰는 방법은 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 파이썬의 주요 내장 함수들은 무엇이 있나요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 for 루프의 기본 구조는 어떻게 되나요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 예외 처리는 어떻게 하나요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 패키지를 설치하는 명령어는 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 파이썬의 주요 데이터 타입에는 무엇이 있나요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 함수는 어떻게 정의하나요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 리스트의 요소를 정렬하는 방법은 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 문자열 포매팅은 어떻게 하나요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 lambda 함수는 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 모듈과 패키지의 차이는 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 글로벌 변수와 로컬 변수의 차이는 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 파이썬의 주요 표준 라이브러리에는 무엇이 있나요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 set 자료형은 어떻게 사용하나요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 map 함수는 어떻게 사용하나요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 데코레이터는 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 제너레이터는 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 리스트 컴프리헨션은 어떻게 사용하나요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 가비지 컬렉션은 어떻게 동작하나요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 멀티스레딩을 구현하는 방법은 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 데이터베이스 연결은 어떻게 하나요? -> 예측된 레이블: no\n",
      "문장: 파이썬에서 JSON 데이터를 처리하는 방법은 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 최근에 본 영화 중 가장 인상 깊었던 것은 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 좋아하는 계절은 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 아침에 주로 먹는 음식은 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 가장 최근에 간 여행지는 어디인가요? -> 예측된 레이블: no\n",
      "문장: 주말에 주로 하는 활동은 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 좋아하는 음악 장르는 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 최근에 읽은 책은 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 가장 좋아하는 스포츠는 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 친구들과 자주 가는 장소는 어디인가요? -> 예측된 레이블: no\n",
      "문장: 가장 좋아하는 색은 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 좋아하는 음료는 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 가장 좋아하는 음식은 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 최근에 본 드라마는 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 취미가 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 가장 좋아하는 꽃은 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 최근에 본 연극이나 뮤지컬은 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 애완동물이 있나요? -> 예측된 레이블: no\n",
      "문장: 가장 좋아하는 과일은 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 집에서 가장 좋아하는 공간은 어디인가요? -> 예측된 레이블: no\n",
      "문장: 가장 기억에 남는 생일 선물은 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 좋아하는 커피 종류는 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 최근에 본 예능 프로그램은 무엇인가요? -> 예측된 레이블: no\n",
      "문장: 주말에 가장 자주 가는 장소는 어디인가요? -> 예측된 레이블: no\n",
      "문장: 가장 좋아하는 운동 선수는 누구인가요? -> 예측된 레이블: no\n",
      "문장: 최근에 본 뉴스 중 가장 기억에 남는 것은 무엇인가요? -> 예측된 레이블: no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from transformers import BertForSequenceClassification\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_model = 'skt/kobert-base-v1'\n",
    "MODEL_NAME = \"./save/model_v4.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 검증할 데이터 정의\n",
    "class QuestionDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "        \n",
    "# 모델 불러오기\n",
    "model = BertForSequenceClassification.from_pretrained(base_model, num_labels=2)  # 모델의 아키텍처 설정\n",
    "model_save_path = MODEL_NAME  # 가중치 불러오기\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "model = model.to(device)\n",
    "\n",
    "# 검증 데이터 준비\n",
    "validation_df = pd.read_excel('./data/valid_set.xlsx')\n",
    "validation_texts = validation_df['question'].tolist()\n",
    "\n",
    "max_len = 128\n",
    "batch_size = 2\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained(base_model)\n",
    "validation_dataset = QuestionDataset(validation_texts, tokenizer, max_len)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 예측 함수 정의\n",
    "def predict(model, data_loader, device):\n",
    "    model = model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            logits = outputs.logits\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# 예측 수행 및 결과 출력\n",
    "predictions = predict(model, validation_loader, device)\n",
    "\n",
    "for text, pred in zip(validation_texts, predictions):\n",
    "    label = \"yes\" if pred == 1 else \"no\"\n",
    "    print(f\"문장: {text} -> 예측된 레이블: {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
