{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "\n",
    "def extract_keywords_yake(text, max_keywords=5):\n",
    "    # YAKE 키워드 추출기 생성\n",
    "    kw_extractor = yake.KeywordExtractor()\n",
    "    # 텍스트에서 키워드 추출\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    # 상위 max_keywords 개의 키워드 반환\n",
    "    return [kw[0] for kw in keywords[:max_keywords]]\n",
    "\n",
    "# 예제 텍스트\n",
    "text = \"KeyBERT is a minimal and easy-to-use keyword extraction technique.\"\n",
    "\n",
    "# YAKE를 사용하여 키워드 추출\n",
    "keywords = extract_keywords_yake(text)\n",
    "print(\"Extracted Keywords:\", keywords)\n",
    "\n",
    "# 검증\n",
    "expected_keywords = [\"KeyBERT\", \"keyword\", \"extraction\", \"technique\", \"easy-to-use\"]\n",
    "print(\"Expected Keywords:\", expected_keywords)\n",
    "\n",
    "# 간단한 검증\n",
    "correct_keywords = set(keywords) & set(expected_keywords)\n",
    "print(\"Correct Keywords:\", correct_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo apt install default-jdk: ubuntu에서 java를 찾을 수 없을 때\n",
    "# !pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"파이썬에서 다중 라인 문자열의 사용 예는 무엇인가요\"]\n",
    "text = ' '.join(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이썬 다중 라인 문자열 사용 예 무엇\n",
      "  (0, 5)\t0.4082482904638631\n",
      "  (0, 0)\t0.4082482904638631\n",
      "  (0, 1)\t0.4082482904638631\n",
      "  (0, 3)\t0.4082482904638631\n",
      "  (0, 4)\t0.4082482904638631\n",
      "  (0, 2)\t0.4082482904638631\n",
      "['다중' '라인' '무엇' '문자열' '사용' '파이썬']\n",
      "[0.4082482904638631, 0.4082482904638631, 0.4082482904638631, 0.4082482904638631, 0.4082482904638631, 0.4082482904638631]\n",
      "['다중', '라인', '무엇', '문자열', '사용', '파이썬']\n"
     ]
    }
   ],
   "source": [
    "# 명사 추출\n",
    "nouns = okt.nouns(text)\n",
    "processed_text = ' '.join(nouns)\n",
    "print(processed_text)\n",
    "\n",
    "# TF-IDF 벡터\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 텍스트를 TF-IDF벡터로 변환\n",
    "tfidf_matrix = vectorizer.fit_transform([processed_text])\n",
    "print(tfidf_matrix)\n",
    "\n",
    "# 단어와 그에 대한 TF-IDF값 얻기\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names)\n",
    "tfidf_values = tfidf_matrix.toarray().tolist()[0]\n",
    "print(tfidf_values)\n",
    "\n",
    "# 높은 TF-IDF값을 가진 단어 순서대로 정렬\n",
    "keywords = [feature_names[i] for i in sorted(range(len(tfidf_values)), key=lambda k: tfidf_values[k], reverse=True)]\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keybert + kiwi\n",
    "- https://datainclude.me/posts/Keybert%EC%99%80_kiwi%ED%98%95%ED%83%9C%EC%86%8C%EB%B6%84%EC%84%9D%EA%B8%B0%EB%A5%BC_%EC%82%AC%EC%9A%A9%ED%95%98%EC%97%AC_%ED%82%A4%EC%9B%8C%EB%93%9C%EC%B6%94%EC%B6%9C_%ED%95%98%EA%B8%B0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keybert\n",
    "# !pip install kiwipiepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('데이터', 0.8142)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from transformers import BertModel\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "text = '데이터프레임에서 특정 열을 추출하려면 어떻게 해야 하나요'\n",
    "\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "kw_model = KeyBERT(model)\n",
    "keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=10)\n",
    "\n",
    "kiwi = Kiwi()\n",
    "kiwi.analyze(text)\n",
    "\n",
    "# 명사 추출 함수\n",
    "def noun_extractor(text):\n",
    "    results = []\n",
    "    result = kiwi.analyze(text)\n",
    "    for token, pos, _, _ in result[0][0]:\n",
    "        if len(token) != 1 and pos.startswith('N') or pos.startswith('SL'):\n",
    "            results.append(token)\n",
    "    return results\n",
    "\n",
    "nouns = noun_extractor(text)\n",
    "\n",
    "text = ' '.join(nouns)\n",
    "\n",
    "keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=1)\n",
    "\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from transformers import BertModel\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# 모델 선택\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "kw_model = KeyBERT(model)\n",
    "kiwi = Kiwi()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('range', 0.6023), ('반복', 0.5649), ('사용', 0.2457), ('함수', 0.2364)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'range() 함수를 사용한 반복문의 예를 들어보세요.'\n",
    "\n",
    "# 명사 추출 함수\n",
    "def noun_extractor(text):\n",
    "    results = []\n",
    "    result = kiwi.analyze(text)\n",
    "    for token, pos, _, _ in result[0][0]:\n",
    "        if len(token) != 1 and pos.startswith('N') or pos.startswith('SL'):\n",
    "            results.append(token)\n",
    "    return results\n",
    "\n",
    "nouns = noun_extractor(text)\n",
    "\n",
    "text = ' '.join(nouns)\n",
    "\n",
    "keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=20)\n",
    "\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "키워드 추출 및 저장이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from transformers import BertModel\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# 모델 선택\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "kw_model = KeyBERT(model)\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# 데이터 불러오기\n",
    "df = pd.read_excel('../fasttext/data/train_set.xlsx')\n",
    "\n",
    "# 불용어 파일 불러오기\n",
    "with open('stopwords.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "# 명사 추출 함수\n",
    "def noun_extractor(text):\n",
    "    results = []\n",
    "    result = kiwi.analyze(text)\n",
    "    for token, pos, _, _ in result[0][0]:\n",
    "        if len(token) != 1 and (pos.startswith('N') or pos.startswith('SL')):\n",
    "            results.append(token)\n",
    "    return results\n",
    "\n",
    "# 키워드 추출 함수\n",
    "def extract_keywords_from_df(df, stopwords, kw_model):\n",
    "    keywords_list = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if row['label'] == 'yes':\n",
    "            noun = noun_extractor(row['question'])\n",
    "            text_noun = ' '.join(noun)\n",
    "            keywords = kw_model.extract_keywords(text_noun, keyphrase_ngram_range=(1, 1), stop_words=stopwords, top_n=20)\n",
    "            keywords_list.append(', '.join([kw[0] for kw in keywords]))\n",
    "        else:\n",
    "            keywords_list.append('') \n",
    "    return keywords_list\n",
    "\n",
    "\n",
    "# 키워드 추출\n",
    "df['keyword'] = extract_keywords_from_df(df, stopwords, kw_model)\n",
    "\n",
    "# 결과를 엑셀 파일로 저장\n",
    "df.to_excel('keyword.xlsx', index=False)\n",
    "\n",
    "print(\"키워드 추출 및 저장이 완료되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/bok/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/home/bok/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrake_nltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rake\n\u001b[0;32m----> 3\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mRake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/rake_nltk/rake.py:84\u001b[0m, in \u001b[0;36mRake.__init__\u001b[0;34m(self, stopwords, punctuations, language, ranking_metric, max_length, min_length, include_repeated_phrases, sentence_tokenizer, word_tokenizer)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopwords \u001b[38;5;241m=\u001b[39m stopwords\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(language))\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# If punctuations are not provided we ignore all punctuation symbols.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpunctuations: Set[\u001b[38;5;28mstr\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/bok/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "r = Rake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/bok/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/home/bok/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrake_nltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rake\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize RAKE\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mRake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Define the text\u001b[39;00m\n\u001b[1;32m      9\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature extraction is not that complex. There are many algorithms available that can help you with feature extraction. Rapid Automatic Key Word Extraction is one of those\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/rake_nltk/rake.py:84\u001b[0m, in \u001b[0;36mRake.__init__\u001b[0;34m(self, stopwords, punctuations, language, ranking_metric, max_length, min_length, include_repeated_phrases, sentence_tokenizer, word_tokenizer)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopwords \u001b[38;5;241m=\u001b[39m stopwords\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(language))\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# If punctuations are not provided we ignore all punctuation symbols.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpunctuations: Set[\u001b[38;5;28mstr\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/bok/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "text = \"Feature extraction is not that complex. There are many algorithms available that can help you with feature extraction. Rapid Automatic Key Word Extraction is one of those\"\n",
    "\n",
    "r.extract_keywords_from_text(text)\n",
    "\n",
    "keywords_rake = r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# GPT-2 모델과 토크나이저 로드\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# GPU 사용 가능 시 GPU로 모델 이동\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def generate_response(question):\n",
    "    inputs = tokenizer.encode(question, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(inputs, max_length=150, num_return_sequences=1)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How do I create a list in Python?\\n\\nYou can create a list using the following syntax:\\n\\nfrom list import list\\n\\nYou can also use the following syntax:\\n\\nfrom list import list\\n\\nYou can also use the following syntax:\\n\\nfrom list import list\\n\\nYou can also use the following syntax:\\n\\nfrom list import list\\n\\nYou can also use the following syntax:\\n\\nfrom list import list\\n\\nYou can also use the following syntax:\\n\\nfrom list import list\\n\\nYou can also use the following syntax:\\n\\nfrom list import list\\n\\nYou can also use the following syntax:\\n\\nfrom list import list\\n\\nYou can also use the following syntax:\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 답변 생성이 아니라 질문에 추가되는 무언가를 생성이잖아\n",
    "generate_response(\"How do I create a list in Python?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['문제', '처리', '최신', '중요', '이스케이프', '튜플의', '키워드', 'interpreter', '텍스트', 'in', '바인딩', 'hex', '기능', 'max', '자료', 'enumerate', '메모리', '사용법', 'binding', '유형', 'constant', 'string', '용도', '해제', 'dictionary', '구분', 'sqlite', '이유', '클래스', '의미', '관습', '중복', 'iterator', '차원', '개수', '삽입', '참조', 'codecs', 'first', 'globals', 'self', 'any', '소개', 'dict', 'copy', 'is', '체크', '정의', 'else', '누락', '파일', 'py', '컴프리헨션', '특정', '최소', '순회', 'bin', 'rule', '시각', 'for', '예약어', '순서', '일급', '라인', '철학', 'scope', 'duck', '람다', '주의', '할당', '데이터', 'locals', '복사', '배열', '지정', '횟수', '내부', '선택', 'dynamic', 'sorted', '타입', '예시', 'datetime', '메타', '통계', '컴파일', 'named', '호출', '인덱스', '객체', '표준', '연산자', 'xml', '반복', '커링', 'defaultdict', '차이', '역할', '콤마', '패킹', 'closure', '카운터', '소수점', 'issubclass', 're', '모델', '타이핑', '제어', '출력', '상황', 'random', '기본', '역순', '라이브러리', 'xhh', 'array', '패키지', 'unittest', '누적', 'min', 'language', '요약', 'not', '정수', '해결', '힌팅', '부울', 'bytes', '형식', '스크립트', 'context', '프레임', 'gil', '엑셀', 'difference', '유지', 'iter', '설치', '이름', '유니코드', 'ternary', '요소', '결합', '치환', '백슬래시', 'product', '포맷팅', '리스트', 'garbage', 'sys', 'fractions', 'itemgetter', '접근', 'filecmp', '관리', '설정', '클로저', '메소드', 'str', '평균', 'init', 'print', '메서', '상수', '실행', 'typing', 'slots', '상속', '기준', 'math', '다양', '반환', '오버로딩', 'set', 'functools', '삭제', 'jit', '인터프리터', '언더', '평균값', '기호', '외부', 'scoping', 'while', 'unpacking', '내포', '작성', 'json', '충족', '딕셔너리', 'subprocess', 'time', 'reversed', '좌우', '변수', 'tuple', '문서', '쓰기', '위치', 'pep', 'path', 'list', '생성', '스페이스', 'union', '가비지', '포함', '필터링', '수정', 'calendar', 'logging', '만족', 'collection', 'range', 'reduce', '모듈', '사용량', 'dir', '강력', '날짜', 'doctest', 'chr', '종료', '파싱', '복합', 'deque', '예외', 'filter', '특징', 'itertools', 'oop', 'yield', 'pow', 'pickle', '야기', '스코핑', '리턴', '구조', '정렬', 'manager', 'counter', '나열', '문법', '컬럼', '결정', 'raw', '크기', '방식', '병합', '최대', '메서드', 'decimal', 'callable', '명명', 'currying', 'decorator', '개행', '문자', '캐리', 'namespace', '스코프를', '슬라이싱', '최저', '조건문', 'frozenset', '선언', '매니저', '강제', 'intersection', 'all', 'shutil', 'docstring', '레거시', '불린', 'hashlib', 'zip', 'main', 'bool', '모드', '제공', '다중', '특수', '프로그래밍', '자료형', '함수', '튜플', '이용', '전역', 'strong', '컴파일러', '합계', 'with', 'none', 'comprehension', '중첩', '결손', 'vars', '범위', 'nan', '테이블', 'object', '비교', '튜플과', 'boolean', 'metaclass', '유니크', '네임', '임포트', '이동', '지향', 'help', '슬라이스', 'elif', '동작', 'class', '블록', '표현', 'break', 'pathlib', '내장', 'bisect', '연산', '주석', '초기', '가지', 'statistics', 'hmac', '동시', '수행', '자릿수', 'expression', '대신', 'overloading', '이터레이터', '루프', '코드', 'len', 'lock', '스코프', 'glob', 'round', '대체', '변환', '계산', '스코어', '무한', '고유', '데코레이터', 'id', 'global', '논리', '최고', 'abs', 'if', '컬렉션', '개념', 'hinting', '포맷', '제거', '예제', '시간', '숫자', 'generator', 'built', 'delattr', '조건', '확인', 'os', 'operator', 'continue', '그룹', '동적', '교환', 'timeit', 'sum', 'argparse', 'series', '명령어', 'getopt', '변경', '언어', 'finally', '카운트', 'format', '제너레이터', 'csv', 'slice', '문자열', '주요', 'compile', '액세스', '저장', '집계', '추출', 'int', '작업', 'uuid', 'divmod', '자리', 'collections', 'ooo', '이진수', 'difflib', '추가', '차이점', '시퀀스', '이하', 'nonlocal', '인자', '지역', '중앙값', '충돌', '규칙', 'input', '리셋', 'type', 'isinstance', 'conditional', 'name', 'pandas', 'map', 'slicing']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 엑셀 파일 불러오기\n",
    "key_df = pd.read_excel('keyword.xlsx')\n",
    "\n",
    "# 결측치 제거\n",
    "key_df = key_df[key_df['keyword'].notna()]\n",
    "\n",
    "# 'keyword' 열을 리스트로 변환\n",
    "keywords = key_df['keyword'].to_list()\n",
    "\n",
    "# 쉼표와 공백으로 구분된 각 키워드를 개별 단어로 분리\n",
    "split_keywords = []\n",
    "for keyword in keywords:\n",
    "    split_keywords.extend([k.strip() for k in keyword.split(',')])\n",
    "\n",
    "# 중복 제거 및 각 단어를 작은따옴표로 감싸기\n",
    "unique_keywords = list(set(split_keywords))\n",
    "quoted_keywords = [f\"'{word}'\" for word in unique_keywords]\n",
    "\n",
    "# 리스트 형태로 표현\n",
    "result = f\"[{', '.join(quoted_keywords)}]\"\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "if '데이터 프레임' in result:\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
