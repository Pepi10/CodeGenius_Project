{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "\n",
    "def extract_keywords_yake(text, max_keywords=5):\n",
    "    # YAKE 키워드 추출기 생성\n",
    "    kw_extractor = yake.KeywordExtractor()\n",
    "    # 텍스트에서 키워드 추출\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    # 상위 max_keywords 개의 키워드 반환\n",
    "    return [kw[0] for kw in keywords[:max_keywords]]\n",
    "\n",
    "# 예제 텍스트\n",
    "text = \"KeyBERT is a minimal and easy-to-use keyword extraction technique.\"\n",
    "\n",
    "# YAKE를 사용하여 키워드 추출\n",
    "keywords = extract_keywords_yake(text)\n",
    "print(\"Extracted Keywords:\", keywords)\n",
    "\n",
    "# 검증\n",
    "expected_keywords = [\"KeyBERT\", \"keyword\", \"extraction\", \"technique\", \"easy-to-use\"]\n",
    "print(\"Expected Keywords:\", expected_keywords)\n",
    "\n",
    "# 간단한 검증\n",
    "correct_keywords = set(keywords) & set(expected_keywords)\n",
    "print(\"Correct Keywords:\", correct_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo apt install default-jdk: ubuntu에서 java를 찾을 수 없을 때\n",
    "# !pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"파이썬에서 다중 라인 문자열의 사용 예는 무엇인가요\"]\n",
    "text = ' '.join(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사 추출\n",
    "nouns = okt.nouns(text)\n",
    "processed_text = ' '.join(nouns)\n",
    "print(processed_text)\n",
    "\n",
    "# TF-IDF 벡터\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 텍스트를 TF-IDF벡터로 변환\n",
    "tfidf_matrix = vectorizer.fit_transform([processed_text])\n",
    "print(tfidf_matrix)\n",
    "\n",
    "# 단어와 그에 대한 TF-IDF값 얻기\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names)\n",
    "tfidf_values = tfidf_matrix.toarray().tolist()[0]\n",
    "print(tfidf_values)\n",
    "\n",
    "# 높은 TF-IDF값을 가진 단어 순서대로 정렬\n",
    "keywords = [feature_names[i] for i in sorted(range(len(tfidf_values)), key=lambda k: tfidf_values[k], reverse=True)]\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keybert + kiwi\n",
    "- https://datainclude.me/posts/Keybert%EC%99%80_kiwi%ED%98%95%ED%83%9C%EC%86%8C%EB%B6%84%EC%84%9D%EA%B8%B0%EB%A5%BC_%EC%82%AC%EC%9A%A9%ED%95%98%EC%97%AC_%ED%82%A4%EC%9B%8C%EB%93%9C%EC%B6%94%EC%B6%9C_%ED%95%98%EA%B8%B0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keybert\n",
    "# !pip install kiwipiepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from transformers import BertModel\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "text = '데이터프레임에서 특정 열을 추출하려면 어떻게 해야 하나요'\n",
    "\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "kw_model = KeyBERT(model)\n",
    "keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=10)\n",
    "\n",
    "kiwi = Kiwi()\n",
    "kiwi.analyze(text)\n",
    "\n",
    "# 명사 추출 함수\n",
    "def noun_extractor(text):\n",
    "    results = []\n",
    "    result = kiwi.analyze(text)\n",
    "    for token, pos, _, _ in result[0][0]:\n",
    "        if len(token) != 1 and pos.startswith('N') or pos.startswith('SL'):\n",
    "            results.append(token)\n",
    "    return results\n",
    "\n",
    "nouns = noun_extractor(text)\n",
    "\n",
    "text = ' '.join(nouns)\n",
    "\n",
    "keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=1)\n",
    "\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from transformers import BertModel\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# 모델 선택\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "kw_model = KeyBERT(model)\n",
    "kiwi = Kiwi()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'range() 함수를 사용한 반복문의 예를 들어보세요.'\n",
    "\n",
    "# 명사 추출 함수\n",
    "def noun_extractor(text):\n",
    "    results = []\n",
    "    result = kiwi.analyze(text)\n",
    "    for token, pos, _, _ in result[0][0]:\n",
    "        if len(token) != 1 and pos.startswith('N') or pos.startswith('SL'):\n",
    "            results.append(token)\n",
    "    return results\n",
    "\n",
    "nouns = noun_extractor(text)\n",
    "\n",
    "text = ' '.join(nouns)\n",
    "\n",
    "keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=20)\n",
    "\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from transformers import BertModel\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# 모델 선택\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "kw_model = KeyBERT(model)\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# 데이터 불러오기\n",
    "df = pd.read_excel('../fasttext/data/train_set.xlsx')\n",
    "\n",
    "# 불용어 파일 불러오기\n",
    "with open('stopwords.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "# 명사 추출 함수\n",
    "def noun_extractor(text):\n",
    "    results = []\n",
    "    result = kiwi.analyze(text)\n",
    "    for token, pos, _, _ in result[0][0]:\n",
    "        if len(token) != 1 and (pos.startswith('N') or pos.startswith('SL')):\n",
    "            results.append(token)\n",
    "    return results\n",
    "\n",
    "# 키워드 추출 함수\n",
    "def extract_keywords_from_df(df, stopwords, kw_model):\n",
    "    keywords_list = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if row['label'] == 'yes':\n",
    "            noun = noun_extractor(row['question'])\n",
    "            text_noun = ' '.join(noun)\n",
    "            keywords = kw_model.extract_keywords(text_noun, keyphrase_ngram_range=(1, 1), stop_words=stopwords, top_n=20)\n",
    "            keywords_list.append(', '.join([kw[0] for kw in keywords]))\n",
    "        else:\n",
    "            keywords_list.append('') \n",
    "    return keywords_list\n",
    "\n",
    "\n",
    "# 키워드 추출\n",
    "df['keyword'] = extract_keywords_from_df(df, stopwords, kw_model)\n",
    "\n",
    "# 결과를 엑셀 파일로 저장\n",
    "df.to_csv('keyword.csv', index=False)\n",
    "\n",
    "print(\"키워드 추출 및 저장이 완료되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실행파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TensorFlow 경고 메시지 억제\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0: all messages, 1: info messages, 2: warnings, 3: errors\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from transformers import BertModel\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# bert와 keybert 모델을 활용하여 문장에서 명사를 추출하는 함수와 키워드를 추출하는 함수가 있는 객체입니다.\n",
    "class Extract_keywords:\n",
    "    def __init__(self, model_name='skt/kobert-base-v1', stopwords=None, top_n=5):  # stopwords와 top_n 기본값 설정\n",
    "        self.model_name = model_name\n",
    "        self.stopwords = stopwords if stopwords else []\n",
    "        self.top_n = top_n\n",
    "        self.kiwi = Kiwi()\n",
    "        self.kw_model = self.load_kw_extractor_model()\n",
    "\n",
    "    # 키워드 추출 모델 불러오는 함수\n",
    "    def load_kw_extractor_model(self):\n",
    "        model = BertModel.from_pretrained(self.model_name)\n",
    "        kw_model = KeyBERT(model)\n",
    "        return kw_model\n",
    "    \n",
    "    # 명사 추출 메서드\n",
    "    def noun_extractor(self, text):\n",
    "        results = []\n",
    "        result = self.kiwi.analyze(text)\n",
    "        for token, pos, _, _ in result[0][0]:\n",
    "            if len(token) != 1 and (pos.startswith('N') or pos.startswith('SL')):\n",
    "                results.append(token)\n",
    "        return results\n",
    "\n",
    "    # 데이터 프레임에서 키워드를 추출하는 메서드\n",
    "    def extract_keywords_from_dataframe(self, dataframe, extract_column_name, label_column_name, label_value):\n",
    "        max_keywords = 0\n",
    "        keywords_list = []\n",
    "        for _, row in dataframe.iterrows():\n",
    "            # 라벨이 0인 컬럼만 불러오기\n",
    "            if row[label_column_name] == label_value:\n",
    "                noun = self.noun_extractor(row[extract_column_name])\n",
    "                text_noun = ' '.join(noun)\n",
    "                keywords = self.kw_model.extract_keywords(text_noun, keyphrase_ngram_range=(1, 1), stop_words=self.stopwords, top_n=self.top_n)\n",
    "                keyword_texts = [kw[0] for kw in keywords]\n",
    "                max_keywords = max(max_keywords, len(keyword_texts))\n",
    "                keywords_list.append(keyword_texts)\n",
    "            else:\n",
    "                keywords_list.append([]) \n",
    "        \n",
    "        # 키워드를 데이터프레임에 저장\n",
    "        for i in range(max_keywords):\n",
    "            col_name = f'keyword_{i+1}'\n",
    "            dataframe[col_name] = [keywords[i] if i < len(keywords) else '픂뽉쌭' for keywords in keywords_list]\n",
    "        return dataframe\n",
    "    \n",
    "    # 문장에서 키워드를 추출하는 메서드\n",
    "    def extract_keywords_from_text(self, text):\n",
    "        noun = self.noun_extractor(text)\n",
    "        text_noun = ' '.join(noun)\n",
    "        keywords = self.kw_model.extract_keywords(text_noun, keyphrase_ngram_range=(1, 1), stop_words=self.stopwords, top_n=self.top_n)\n",
    "        return [kw[0] for kw in keywords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from transformers import BertModel\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# bert와 keybert 모델을 활용하여 문장에서 명사를 추출하는 함수와 키워드를 추출하는 함수가 있는 객체입니다.\n",
    "class Extract_keywords:\n",
    "    def __init__(self, model_name='skt/kobert-base-v1', stopwords=None, top_n=5):  # stopwords와 top_n 기본값 설정\n",
    "        self.model_name = model_name\n",
    "        self.stopwords = stopwords if stopwords else []\n",
    "        self.top_n = top_n\n",
    "        self.kiwi = Kiwi()\n",
    "        self.kw_model = self.load_kw_extractor_model()\n",
    "\n",
    "    # 키워드 추출 모델 불러오는 함수\n",
    "    def load_kw_extractor_model(self):\n",
    "        model = BertModel.from_pretrained(self.model_name)\n",
    "        kw_model = KeyBERT(model)\n",
    "        return kw_model\n",
    "    \n",
    "    # 명사 추출 메서드\n",
    "    def noun_extractor(self, text):\n",
    "        results = []\n",
    "        result = self.kiwi.analyze(text)\n",
    "        for token, pos, _, _ in result[0][0]:\n",
    "            if len(token) != 1 and (pos.startswith('N') or pos.startswith('SL')):\n",
    "                results.append(token)\n",
    "        return results\n",
    "\n",
    "    # 데이터 프레임에서 키워드를 추출하는 메서드\n",
    "    def extract_keywords_from_dataframe(self, dataframe, extract_column_name, label_column_name, label_value):\n",
    "        max_keywords = 0\n",
    "        keywords_list = []\n",
    "        for _, row in dataframe.iterrows():\n",
    "            # 라벨이 0인 컬럼만 불러오기\n",
    "            if row[label_column_name] == label_value:\n",
    "                noun = self.noun_extractor(row[extract_column_name])\n",
    "                text_noun = ' '.join(noun)\n",
    "                keywords = self.kw_model.extract_keywords(text_noun, keyphrase_ngram_range=(1, 1), stop_words=stopwords, top_n=self.top_n)\n",
    "                keyword_texts = [kw[0] for kw in keywords]\n",
    "                max_keywords = max(max_keywords, len(keyword_texts))\n",
    "                keywords_list.append(keyword_texts)\n",
    "            else:\n",
    "                keywords_list.append([]) \n",
    "        \n",
    "        # 키워드를 데이터프레임에 저장\n",
    "        for i in range(max_keywords):\n",
    "            col_name = f'keyword_{i+1}'\n",
    "            dataframe[col_name] = [keywords[i] if i < len(keywords) else '픂뽉쌭' for keywords in keywords_list]\n",
    "        return dataframe\n",
    "    \n",
    "    # 문장에서 키워드를 추출하는 메서드\n",
    "    def extract_keywords_from_text(self, text):\n",
    "        noun = self.noun_extractor(text)\n",
    "        text_noun = ' '.join(noun)\n",
    "        keywords = self.kw_model.extract_keywords(text_noun, keyphrase_ngram_range=(1, 1), stop_words=self.stopwords, top_n=self.top_n)\n",
    "        return [kw[0] for kw in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "df = pd.read_excel('../fasttext/data/train_set.xlsx')\n",
    "\n",
    "# 불용어 파일 불러오기\n",
    "with open('stopwords.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "# Class 인스턴스 생성 \n",
    "extractor = Extract_keywords(stopwords=stopwords, top_n=5)\n",
    "\n",
    "# 키워드 추출하여 데이터프레임 업데이트\n",
    "df = extractor.extract_keywords_from_dataframe(df, extract_column_name='question', label_column_name='label', label_value='yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확인\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv파일로 내보내기\n",
    "df.to_csv('keyword.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단일 텍스트\n",
    "# 명사만 추출\n",
    "text_noun = extractor.noun_extractor(\"판다스 데이터 프레임에대해 설명해주세요\")\n",
    "# 키워드 추출\n",
    "text_kw = extractor.extract_keywords_from_text(\"판다스 데이터 프레임에대해 설명해주세요\")\n",
    "\n",
    "print(text_noun)\n",
    "print(text_kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 엑셀 파일 불러오기\n",
    "key_df = pd.read_excel('keyword.xlsx')\n",
    "\n",
    "# 결측치 제거\n",
    "key_df = key_df[key_df['keyword'].notna()]\n",
    "\n",
    "# 'keyword' 열을 리스트로 변환\n",
    "keywords = key_df['keyword'].to_list()\n",
    "\n",
    "# 쉼표와 공백으로 구분된 각 키워드를 개별 단어로 분리\n",
    "split_keywords = []\n",
    "for keyword in keywords:\n",
    "    split_keywords.extend([k.strip() for k in keyword.split(',')])\n",
    "\n",
    "# 중복 제거 및 각 단어를 작은따옴표로 감싸기\n",
    "unique_keywords = list(set(split_keywords))\n",
    "quoted_keywords = [f\"'{word}'\" for word in unique_keywords]\n",
    "\n",
    "# 리스트 형태로 표현\n",
    "result = f\"[{', '.join(quoted_keywords)}\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '데이터 프레임' in result:\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
